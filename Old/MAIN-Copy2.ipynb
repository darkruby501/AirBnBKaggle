{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN\n",
    "This is the master notebook for my AirBnB Recruiting Challenge work. Changes and additions are made by branching and merging.\n",
    "\n",
    "Other possible branches:\n",
    "* Explore\n",
    "* Prepocessing\n",
    "* Features\n",
    "* Models\n",
    "* Multiclass\n",
    "* Imbalance\n",
    "* Validation\n",
    "* Ensembels\n",
    "* Imputation\n",
    "* Test\n",
    "* PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Draw inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set figure aesthetics\n",
    "sns.set_style(\"whitegrid\") #, {'ytick.major.size': 10.0})\n",
    "#sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data into DataFrames\n",
    "train_users = pd.read_csv('input/train_users_2.csv')\n",
    "test_users = pd.read_csv('input/test_users.csv')\n",
    "\n",
    "piv_train = train_users.shape[0]\n",
    "labels = train_users['country_destination'].values\n",
    "\n",
    "\n",
    "all_users = pd.concat([train_users,test_users],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Measure: NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rank_metrics import ndcg_at_k\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def ndcg_wrapper(y_true,y_pred_proba):\n",
    "    \n",
    "    Y = np.fliplr(y_pred_proba.argsort())\n",
    "        \n",
    "    R = []\n",
    "    NDCG = []\n",
    "    for i in range(0,y_true.size):\n",
    "        r = (Y[i,:]==y_true[i]).astype(int)\n",
    "        R.append(r)\n",
    "        #ndcg_at_k([0,0,1],5,method=1)\n",
    "        NDCG.append(ndcg_at_k(r,5,method=1))\n",
    "    \n",
    "    #print(NDCG)\n",
    "    #return NDCG,R\n",
    "    return np.mean(NDCG)\n",
    "\n",
    "ndcg_scorer = make_scorer(ndcg_wrapper, greater_is_better=True, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Main Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_missing(df):\n",
    "    ##AGE\n",
    "    \n",
    "    ## DISCRETIZE AGE\n",
    "    A = pd.cut(df['age'],list(range(14,100,5)),right=True)\n",
    "    A = pd.get_dummies(A)\n",
    "    df= pd.concat([df,A],axis=1)\n",
    "    \n",
    "    #Remove all ages outside of range, set to -1 for separate categorisation //impute for logistic regression\n",
    "    df.loc[df.age > 100, 'age'] = np.nan\n",
    "    df.loc[df.age < 15, 'age'] = np.nan\n",
    "    df['age'].fillna(-111,inplace=True)\n",
    "\n",
    "    ## GENDER\n",
    "    # Set missing values to own category\n",
    "    df['gender'].replace('-unknown-',np.nan, inplace=True)\n",
    "    #df['gender'].fillna('MISSING',inplace=True)\n",
    "\n",
    "    ## FIRST AFFILIATE TRACKED\n",
    "    # Set missing to untracked, hopefully the same\n",
    "    df['first_affiliate_tracked'].fillna('untracked',inplace=True)\n",
    "    \n",
    "    ## Get rid of date_first_booking\n",
    "    if 'date_first_booking' in df.columns:\n",
    "        df.drop(['date_first_booking'],axis=1,inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_features(df):\n",
    "    ## dates\n",
    "    \n",
    "    # date_account_created\n",
    "    dac = np.vstack(df.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\n",
    "    df['dac_year'] = dac[:,0]\n",
    "    df['dac_month'] = dac[:,1]\n",
    "    df['dac_day'] = dac[:,2]\n",
    "    df.drop(['date_account_created'],axis=1,inplace=True)\n",
    "    \n",
    "    #time first active\n",
    "    tfa = np.vstack(df.timestamp_first_active.astype(str).apply(lambda x: list(map(int, [x[:4],x[4:6],x[6:8],x[8:10],x[10:12],x[12:14]]))).values)\n",
    "    df['tfa_year'] = tfa[:,0]\n",
    "    df['tfa_month'] = tfa[:,1]\n",
    "    df['tfa_day'] = tfa[:,2]\n",
    "    df['tfa_hour'] = tfa[:,3]\n",
    "    df.drop(['timestamp_first_active'],axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    ## One-Hot Encoding\n",
    "    categorical_features = ['affiliate_channel','affiliate_provider','first_affiliate_tracked',\n",
    "                            'first_browser','first_device_type','gender','language','signup_app','signup_flow','signup_method'\n",
    "                           ]\n",
    "    \n",
    "    df = pd.get_dummies(df,columns=categorical_features)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_all(df):\n",
    "\n",
    "    df_p = preprocess_missing(df)\n",
    "    df_p = preprocess_features(df_p)\n",
    "    \n",
    "    return(df_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "All = preprocess_all(all_users) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session_df = pd.read_csv('input/sessions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts of Actions Taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session_users = session_df\n",
    "\n",
    "# Clean missing values\n",
    "session_users['secs_elapsed'].fillna(0,inplace=True)\n",
    "session_users['action_type'].fillna('',inplace=True)\n",
    "\n",
    "# Combine for grouping\n",
    "#session_users['action_action_type'] = session_users['action']+'__'+session_users['action_type']\n",
    "session_users['action_action_detail'] = session_users['action']+'__'+session_users['action_detail']\n",
    "#session_users['action_action_type_detail'] = session_users['action']+'__'+session_users['action_type']+'__'+session_users['action_detail']\n",
    "\n",
    "\n",
    "# Group actions for users\n",
    "#session_users1 = session_users.groupby(['user_id','action_action_type']).count()['secs_elapsed'].unstack().fillna(0)\n",
    "session_users2 = session_users.groupby(['user_id','action_action_detail']).count()['secs_elapsed'].unstack().fillna(0)\n",
    "#session_users3 = session_users.groupby(['user_id','action_action_type_detail']).count()['secs_elapsed'].unstack().fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#session_users2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elapsed Time by Device Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deviceusage_df = session_df[session_df['secs_elapsed']<60*100][['user_id','device_type','secs_elapsed']].groupby(['user_id','device_type']).sum().unstack()['secs_elapsed']\n",
    "deviceusage_df.fillna(0,inplace=True)\n",
    "#deviceusage_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SessionFeatures = pd.merge(session_users3,deviceusage_df,right_index=True,left_index=True,how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SessionFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Session Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 150\n",
    "\n",
    "#Need to use labelled to select\n",
    "SessionX = pd.merge(train_users[['id','country_destination']],SessionFeatures,right_index=True,left_on='id',how='inner')\n",
    "Sessiony = SessionX['country_destination']\n",
    "SessionX.drop(['id','country_destination'],axis=1,inplace=True)\n",
    "\n",
    "#test_users2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "selector = SelectKBest(k=150)\n",
    "selector.fit(SessionX,Sessiony)\n",
    "SessionFeatures = SessionFeatures[SessionFeatures.columns[selector.get_support()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Some More Session Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_session_actions = session_df[session_df['secs_elapsed']<60*100].groupby('user_id')\n",
    "AA = in_session_actions['secs_elapsed'].agg({'total_session_times': np.sum,\n",
    "                                        'mean_secs_per_action' : np.mean,\n",
    "                                            'num_actions_insession' : np.size})\n",
    "\n",
    "between_session_actions = session_df[session_df['secs_elapsed']>200*60].groupby('user_id')\n",
    "BB = between_session_actions['secs_elapsed'].agg({'num_sessions': np.size,\n",
    "                                            'mean_between_sessions' : np.mean,\n",
    "                                            'max_between_sessions': np.max\n",
    "                                            })\n",
    "\n",
    "SessionFeatures2 = pd.merge(AA,BB,how='outer',left_index=True,right_index=True)\n",
    "\n",
    "\n",
    "SessionFeatures2['num_days'] = session_df[session_df['secs_elapsed']>18*3600].groupby('user_id').count()['secs_elapsed']\n",
    "SessionFeatures2['num_devices'] = session_df.groupby('user_id')['device_type'].agg(lambda x: len(np.unique(x.values)))\n",
    "SessionFeatures2.fillna(0,inplace=True)\n",
    "SessionFeatures2['num_sessions'] += 1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SessionFeatures2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SessionFeatures = SessionFeatures.merge(SessionFeatures2,right_index=True,left_index=True,how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age_Gender_Bkts - Create Features\n",
    "Could also add in Country Locations here . . . if that makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_gender_df = pd.read_csv('input/age_gender_bkts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A = pd.pivot_table(age_gender_df,values='population_in_thousands',index=['age_bucket','gender'],columns='country_destination')\n",
    "normalised_brackets = A.divide(A.sum(axis=1),axis=0)\n",
    "normalised_totals = A.sum()/A.sum().sum()\n",
    "#normalised_brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#AA = A.reset_index().groupby('gender').sum()  ## - Later separate missing by male and female . . . very sligth difference?\n",
    "#AA.loc['female']/AA.loc['male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def age_bucket_converter(age, gender):\n",
    "    \n",
    "    \n",
    "    if (age>0)&(age<100):\n",
    "        a = int((age//5)*5)\n",
    "        Age = str(a)+'-'+str(a+4)\n",
    "        if Age == '5-9': Age = 'Missing'\n",
    "        \n",
    "    elif age>99: Age = '100+'\n",
    "    else: Age = 'Missing'\n",
    "        \n",
    "    if gender == 'MALE': Gender = 'male'\n",
    "    elif gender == 'FEMALE': Gender = 'female'\n",
    "    else: Gender = 'Missing'\n",
    "    \n",
    "    return (Age, Gender)\n",
    "\n",
    "def bkts_feature_create(age_gender,normalised_brackets,normalised_totals):\n",
    "     \n",
    "    Age,Gender = age_bucket_converter(*age_gender)\n",
    "    #print([Age,Gender])\n",
    "    \n",
    "    if ((Age=='Missing')|(Gender=='Missing')):\n",
    "        return normalised_totals\n",
    "    else:\n",
    "        return normalised_brackets.loc[(Age,Gender)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BracketFeatures = all_users[['age','gender']].apply(lambda x: bkts_feature_create(tuple(x.values),normalised_brackets,normalised_totals),axis=1)\n",
    "BracketFeatures.set_index(all_users['id'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT AND COMBINE FEATURES ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Merge with Bracket Features\n",
    "All = All.merge(BracketFeatures,how='inner',left_on='id',right_index=True) \n",
    "\n",
    "## Merge with Session Features\n",
    "All = All.merge(SessionFeatures,how='inner',left_on='id',right_index=True)\n",
    "\n",
    "print('num users total = ',All.shape[0])\n",
    "print('num features = ',All.shape[1])\n",
    "\n",
    "# Reattach Ids\n",
    "#All['id'] = all_users['id']\n",
    "#X = X.merge(SessionFeatures,how='inner',left_on='id',right_index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT TRAINING EXAMPLES - Which classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "#included_destinations = ['NDF','US']\n",
    "#included_destinations = ['NDF','US','FR','IT','other'] #,'GB','ES']\n",
    "included_destinations = ['FR','IT','GB','ES']\n",
    "\n",
    "X  = All[All['country_destination'].apply(lambda x: x in included_destinations)]\n",
    "y = le.fit_transform(X['country_destination'])\n",
    "\n",
    "X = X.drop(['country_destination','id'],axis=1)\n",
    "\n",
    "X_Test = All[All['country_destination'].isnull()].drop(['country_destination','id'],axis=1)\n",
    "X_Test_ids = All[All['country_destination'].isnull()]['id']\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "print(list(zip(le.classes_,range(0,len(y)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "#included_destinations = ['NDF','US']\n",
    "#included_destinations = ['NDF','US','FR','IT','other'] #,'GB','ES']\n",
    "\n",
    "\n",
    "X  = All[All['country_destination']!='NDF'].dropna()\n",
    "y = le.fit_transform(X['country_destination'])\n",
    "print(list(zip(le.classes_,range(0,len(y)))))\n",
    "y = (X['country_destination']!='US').astype(int)\n",
    "X = X.drop(['country_destination','id'],axis=1)\n",
    "print(X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "All[All['country_destination'].apply(lambda x: x in included_destinations)].groupby('country_destination').count()['id'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 87.8 with undersampling + SMOTE + new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Custom CV Loop\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from unbalanced_dataset import UnderSampler, SMOTE\n",
    "\n",
    "RF = RandomForestClassifier(oob_score=True,n_estimators=100,n_jobs=-1,class_weight='balanced')\n",
    "Xsave = X\n",
    "ysave = y\n",
    "\n",
    "cv = 5 #number of folds\n",
    "cv_score = []\n",
    "\n",
    "for i in range(0,cv):\n",
    "    X_train, X_test, y, y_test = train_test_split(Xsave, ysave, test_size=0.2,random_state=i,stratify=ysave)\n",
    "    \n",
    "    US = UnderSampler(ratio = 1,verbose=False)\n",
    "    #SMT = SMOTE(ratio=20,kind='regular',verbose=False)\n",
    "    X_us,y_us = US.fit_transform(X_train.values,y)\n",
    "    #X_sm,y_sm = SMT.fit_transform(X_us,y_us)\n",
    "    RF.fit(X_us,y_us)\n",
    "    cv_score.append(ndcg_scorer(RF,X_test,y_test))\n",
    "\n",
    "cv_score\n",
    "print('mean cv score = ',np.mean(cv_score))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('mean cv score = ',np.mean(cv_score))    \n",
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MODELS -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Dummy Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compare with basic. If you can't beat this, there's a probelm.\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "Dummy = DummyClassifier(strategy='prior').fit(X,y)\n",
    "dummy_score = ndcg_scorer(Dummy,X,y)\n",
    "dummy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compare with basic. If you can't beat this, there's a probelm.\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "Dummy = DummyClassifier(strategy='prior').fit(X,y)\n",
    "\n",
    "accuracy_score(y,Dummy.predict(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1 - sum(y)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression\n",
    "Logistic Regression without Grid Search on gets CrossVal of 0.657, which is a mild improvement over 0.583 for setting all NDF. Balancing classes brings it down negligably to 0.646\n",
    "Grid search doesn't seemed to have helped - 0.658 with C of 11.94\n",
    "After scaling age, this now appears third largest among the coefficients, though seemingly a decrease in accuracy to 0.63\n",
    "\n",
    "Discretizing age has increased the score to 0.667, on 1% up. It's something.But none of the age variables features in important coefficients.\n",
    "Reintoducing age as continuous (with NaNs imputed) together with discrete , brings things now up to 0.675. Huh\n",
    "\n",
    "Interesting. The score on the training data is almost the same. So we might not be overfitting and just have too high bias.\n",
    "\n",
    "And now the age brackets are appearing in the coeffs. How very strange. Seems that having entered your age validly correlates with actually booking. Now gender isn't showing up though. Could there just be the single latent variable of \"filling out the form properly?\"\n",
    "\n",
    "Next run of GridSearch gives ~5 for C, stronger regularisation, and a score of 0.688, which is several points up from when I started.\n",
    "\n",
    "----------------- \n",
    "\n",
    "Changing now to multiclass, helps with developing NDCG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Customise Train and Test for Logistic Regression\n",
    "\n",
    "def lr_preprocess(X):\n",
    "    # Accepts already processed X and y\n",
    "\n",
    "    ## DISCRETIZE AGE\n",
    "    #X_log = X # Create X just for Logistic Regression\n",
    "    #A = pd.cut(X_log['age'],list(range(14,100,5)),right=True)\n",
    "    #A = pd.get_dummies(A)\n",
    "    #X_log= pd.concat([X_log,A],axis=1)\n",
    "    #X_train.drop(['age'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "    ## IMPUTE VALUES AND SCALE\n",
    "    X['age'].replace(-111,np.median(X['age']),inplace=True)\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X['age'] = scaler.fit_transform(X['age'])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_log = lr_preprocess(X)\n",
    "X_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%timeit -r1\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(Cs=5,class_weight='balanced') #class_weight='balanced'\n",
    "clf.fit(X_log,y_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C = 0.0045,class_weight='balanced')\n",
    "print(np.mean(cross_val_score(clf,X_log,y_log,cv=5,scoring=ndcg_scorer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%timeit -r1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C = 0.0005,class_weight='balanced')\n",
    "\n",
    "#Need to stratify.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_log, y, random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('NDCG = ',ndcg_scorer(clf,X_test,y_test))\n",
    "print('Accuracy = ',metrics.accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Test for overfitting:\n",
    "y_pred = clf.predict(X_train)\n",
    "print('NDCG = ',ndcg_scorer(clf,X_train,y_train))\n",
    "print('Accuracy = ',metrics.accuracy_score(y_train,y_pred))\n",
    "print('Recall = ',metrics.recall_score(y_train,y_pred))\n",
    "print('Precision = ',metrics.precision_score(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Crange = np.logspace(-1,2,30)\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid={'C': Crange},scoring='accuracy',cv=3,n_jobs=4)\n",
    "grid.fit(X_train,y_train)\n",
    "print (\"best parameter choice:\", grid.best_params_)\n",
    "print (\"best score:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FeatureImportances = pd.DataFrame(data = { 'coefs': clf.coef_.squeeze(), 'abs coefs' : np.abs(clf.coef_.squeeze())},index=X_log.columns)\n",
    "FeatureImportances.sort_values(by='abs coefs',ascending=False,inplace=True)\n",
    "\n",
    "k = 50\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(y=FeatureImportances.index[0:k],x=FeatureImportances['coefs'].head(k))\n",
    "#plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture I get from this that people are less likely to book from mobile devices. And more likely to book if they correctly filled out their from. I doubt logicst regression has handled the age feature well. Hence why discretizing might be a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very first attempt got 0.8449, which is negligibly higher than the dummy estimator at 0.84055\n",
    "\n",
    "I'm confused, why does my simple train test split give 0.82 while the GridSearch gets a mere 0.75\n",
    "\n",
    "Adding in session features seems to help a lot. First train-test split gives 0.8835. Hmm, note that that one isn't weighted.\n",
    "0.879 with threefold cross validation. Looks good, gives a 3.1% improvement over dummy, which is something.\n",
    "\n",
    "Looking at confusion matrix, everything is either US or NDF, never predict anything as Italy, France, or Other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Doesn't require any yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train on all data (for LB submission)\n",
    "RF = RandomForestClassifier(oob_score=True,n_estimators=200,n_jobs=-1,class_weight='balanced')\n",
    "RF.fit(X,y)\n",
    "clf = RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Standalone\n",
    "RF = RandomForestClassifier(oob_score=True,n_estimators=250,max_features=30,n_jobs=-1,class_weight='balanced')\n",
    "RF.fit(X_train,y_train)\n",
    "y_pred = RF.predict(X_test)\n",
    "\n",
    "clf = RF\n",
    "\n",
    "ndcg_scorer(RF,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "RF = RandomForestClassifier(n_estimators=200,max_features=30,n_jobs=-1) #,class_weight='balanced')\n",
    "CV_score = cross_val_score(RF,X,y,scoring=ndcg_scorer, cv=20, verbose=2)\n",
    "print('CV scores = ',CV_score)\n",
    "print('Mean CV score = ', np.mean(CV_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## GridSearch\n",
    "RF = RandomForestClassifier(class_weight='balanced',oob_score=True)\n",
    "gridRF = GridSearchCV(RF,param_grid={'n_estimators':[200],\n",
    "                                   'max_features':[30]}, #\n",
    "                      scoring=ndcg_scorer,cv=4)\n",
    "gridRF.fit(X,y)\n",
    "\n",
    "print(gridRF.best_params_)\n",
    "print(gridRF.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_best = gridRF.best_estimator_\n",
    "RF_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dummy_score = ndcg_scorer(Dummy,X_test,y_test)\n",
    "ndcg_score = ndcg_scorer(RF,X_test,y_test)\n",
    "print('ndcg = ', ndcg_score)\n",
    "print('dummy = ', dummy_score)\n",
    "print('improvement over dummy: %3.3f%%' % ((ndcg_score - dummy_score)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Classificaiton Report\n",
    "clf = RF\n",
    "y_pred = clf.predict(X_test) \n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "#print('ndcg = ', ndcg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "#cm = confusion_matrix(y, clf.predict(X))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('confusion matrix')\n",
    "print(cm)\n",
    "classes=le.classes_\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FeatureImportances = pd.Series(index=X.columns,data=clf.feature_importances_).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 100\n",
    "plt.figure(figsize=(10,20))\n",
    "sns.barplot(y=FeatureImportances.index[0:k],x=FeatureImportances.head(k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(clf, X, y,scoring=ndcg_scorer,train_sizes=[0.1,0.5,0.8,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_learning_curve import plot_learning_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\") #, {'ytick.major.size': 10.0})\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "plt.figure()\n",
    "#plt.title(title)\n",
    "#if ylim is not None: plt.ylim((0.7,1.01)\n",
    "#plt.xlabel = 'H'\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"NDCG Score\")\n",
    "\n",
    "#train_sizes, train_scores, test_scores = learning_curve(    estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(valid_scores, axis=1)\n",
    "test_scores_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "train_scores, valid_scores = validation_curve(RandomForestClassifier(oob_score=True,n_jobs=-1,class_weight='balanced')\n",
    "                                              , X, y, \"n_estimators\", [50,100,200,500],cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(valid_scores, axis=1)\n",
    "test_scores_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "param_range = [50,100,200,500]\n",
    "\n",
    "plt.title(\"Validation Curve with RF\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.plot(param_range, train_scores_mean,'ro', label=\"Training score\", color=\"r\")\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "plt.plot(param_range, test_scores_mean,'go', label=\"Cross-validation score\",\n",
    "             color=\"g\")\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "train_scores, valid_scores = validation_curve(RandomForestClassifier(n_jobs=-1,class_weight='balanced')\n",
    "                                              , X, y, \"max_features\", [10,20,50],cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(valid_scores, axis=1)\n",
    "test_scores_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "param_range = [10,20,50]\n",
    "\n",
    "plt.title(\"Validation Curve with RF\")\n",
    "plt.xlabel(\"max_features\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.plot(param_range, train_scores_mean,'ro', label=\"Training score\", color=\"r\")\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "plt.plot(param_range, test_scores_mean,'go', label=\"Cross-validation score\",\n",
    "             color=\"g\")\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ExtraTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"C:\\Anaconda3\\pkgs\\XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST-SET PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sub(preds,id_test):\n",
    "##Format predictions for Kaggle online submission\n",
    "    \n",
    "    n = 5\n",
    "\n",
    "    ids = [] # list of id's\n",
    "    cts = [] # list of countries\n",
    "    for i in range(len(id_test)):\n",
    "        idx = id_test[i]\n",
    "        ids += [idx]*n\n",
    "        cts += list(preds[i][0:n])\n",
    "    \n",
    "    submission = pd.DataFrame(np.column_stack((ids,cts)), columns=['id','country'])\n",
    "    return submission\n",
    "\n",
    "    \n",
    "#submission.to_csv('output/sub_lr_2class_usndf_plus3_main_bkts.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - US and NDF - Main + Age_Bracket Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_log.head()\n",
    "X_log_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_log.shape)\n",
    "print(X_log_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_test = clf.predict_proba(X_Test)\n",
    "Y = np.fliplr(y_pred_test.argsort()) #Numbers of predicted classes in order of likelihood.\n",
    "y_preds = le.inverse_transform(Y) #Converted to string labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Shoulde turn all of this into a function\n",
    "\n",
    "n = 5\n",
    "\n",
    "id_test = X_Test_ids.values\n",
    "\n",
    "ids = [] # list of id's\n",
    "cts = [] # list of countries\n",
    "for i in range(len(id_test)):\n",
    "    idx = id_test[i]\n",
    "    ids += [idx]*n\n",
    "    cts += list(y_preds[i]) + ['other','FR','IT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(np.column_stack((ids,cts)), columns=['id','country'])\n",
    "submission.to_csv('output/sub_lr_2class_usndf_plus3_main_bkts_UNDERSAMPLING.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest - 5 Top Countries - Including Feature Selected Session + Age_Gender_Bckts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=300,max_features=30,n_jobs=-1,class_weight='balanced')\n",
    "print(clf)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf=RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#clf.fit(X,y)\n",
    "test_users_proba = RF.predict_proba(X_Test)\n",
    "test_users_preds = le.inverse_transform(np.fliplr(test_users_proba.argsort()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_users_preds[1][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = create_sub(test_users_preds,X_Test_ids.values)\n",
    "submission.to_csv('output/sub_rf_5class_top_main_bkts_sf2_balanced.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# BELOW HERE IS MESSY - OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "forest = ExtraTreesClassifier(n_estimators=50,\n",
    "                              random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "forest.fit(X_log, y_int)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_log.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_log.shape[1]), indices)\n",
    "plt.xlim([-1, X_log.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_log.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_log.shape[1]), indices)\n",
    "plt.xlim([-1, X_log.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's get a classifier going\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_log, y_log, random_state=0)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=25)\n",
    "clf.fit(Xtrain,ytrain)\n",
    "ypred = clf.predict(Xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FeatureImportances = pd.Series(index=X_log.columns,data=clf.feature_importances_).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 50\n",
    "plt.figure(figsize=(10,15))\n",
    "sns.barplot(y=FeatureImportances.index[0:k],x=FeatureImportances.head(k))\n",
    "#plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 50\n",
    "plt.figure(figsize=(10,15))\n",
    "sns.barplot(y=FeatureImportances.index[0:k],x=FeatureImportances.head(k))\n",
    "#plt.xticks(rotation = 90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
